# Core logic of the chatbot:
# - Text normalization
# - Rule-based entity extraction
# - Intent & sentiment handling
# - Response generation
# - Model loading and inference

import torch
import re
from pathlib import Path

from .model import ChatbotModel
from .tokenizer import Tokenizer
from .config import DEVICE, MODEL_PATH, MAX_LEN, INTENTS, SENTIMENTS


# -----------------------------
# 1) Text Normalization Utilities
# -----------------------------
def normalize_fa(text: str) -> str:
    """
    Normalize Persian text by:
    - Stripping leading/trailing spaces
    - Unifying Arabic/Persian characters
    - Collapsing multiple spaces
    """
    if not text:
        return ""
    text = text.strip()
    # Unify Arabic and Persian characters
    text = text.replace("ÙŠ", "ÛŒ").replace("Ùƒ", "Ú©")
    # Remove extra whitespaces
    text = re.sub(r"\s+", " ", text)
    return text


# -----------------------------
# 2) Simple Rule-Based Entity Extractor (Regex / Keyword)
# -----------------------------
FACILITIES = [
    "Ø¢Ø³Ø§Ù†Ø³ÙˆØ±", "Ø¨Ø±Ù‚", "Ø¢Ø¨", "Ú¯Ø§Ø²",
    "Ù¾Ø§Ø±Ú©ÛŒÙ†Ú¯", "Ø¯Ø±Ø¨", "Ø¯Ø±", "Ø¯ÙˆØ±Ø¨ÛŒÙ†",
    "Ù„Ø§Ø¨ÛŒ", "Ø§Ø³ØªØ®Ø±", "Ø³Ø§Ù„Ù†", "Ø¨Ø§Ø´Ú¯Ø§Ù‡",
    "Ø±ÙˆÙ", "Ø±ÙˆÙ Ú¯Ø§Ø±Ø¯Ù†", "Ø­ÛŒØ§Ø·", "ØªØ§Ø³ÛŒØ³Ø§Øª"
]

DATE_WORDS = [
    "Ø§Ù…Ø±ÙˆØ²", "ÙØ±Ø¯Ø§", "Ù¾Ø³ ÙØ±Ø¯Ø§",
    "Ø¬Ù…Ø¹Ù‡", "Ø´Ù†Ø¨Ù‡", "ÛŒÚ©Ø´Ù†Ø¨Ù‡",
    "Ø¯ÙˆØ´Ù†Ø¨Ù‡", "Ø³Ù‡ Ø´Ù†Ø¨Ù‡", "Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡", "Ù¾Ù†Ø¬Ø´Ù†Ø¨Ù‡"
]

PRIORITY_HIGH = ["Ø³Ø±ÛŒØ¹", "ÙÙˆØ±ÛŒ", "Ø§Ø¶Ø·Ø±Ø§Ø±ÛŒ", "Ø²ÙˆØ¯", "Ù‡Ù…ÛŒÙ† Ø§Ù„Ø§Ù†"]


def extract_entities(text: str) -> dict:
    """
    Extract basic entities (facility, date, priority)
    using keyword matching on normalized text.
    """
    text_n = normalize_fa(text)
    ents = {}

    # Facility extraction
    fac = []
    for f in FACILITIES:
        if f in text_n:
            fac.append(f)
    if fac:
        ents["facility"] = list(dict.fromkeys(fac))

    # Date extraction
    dates = []
    for d in DATE_WORDS:
        if d in text_n:
            dates.append(d)
    if dates:
        ents["date"] = list(dict.fromkeys(dates))

    # Priority detection (binary: high)
    pri = []
    for p in PRIORITY_HIGH:
        if p in text_n:
            pri.append("high")
            break
    if pri:
        ents["priority"] = pri

    return ents


# -----------------------------
# 3) Rule-Based Response Generator
# -----------------------------
GREETINGS = [
    "Ø³Ù„Ø§Ù…", "Ø³Ù„Ø§Ù…Ù…", "Ø¯Ø±ÙˆØ¯", "ÙˆÙ‚Øª Ø¨Ø®ÛŒØ±",
    "ØµØ¨Ø­ Ø¨Ø®ÛŒØ±", "Ø¹ØµØ± Ø¨Ø®ÛŒØ±", "Ø´Ø¨ Ø¨Ø®ÛŒØ±",
    "Ø®Ø³ØªÙ‡ Ù†Ø¨Ø§Ø´ÛŒØ¯", "hi", "hello"
]

THANKS = [
    "Ù…Ù…Ù†ÙˆÙ†", "Ù…Ø±Ø³ÛŒ", "Ø¯Ù…Øª Ú¯Ø±Ù…", "Ø³Ù¾Ø§Ø³"
]


def is_greeting(text: str) -> bool:
    """Check whether input text is a greeting."""
    t = normalize_fa(text).lower()
    return any(g in t for g in GREETINGS)


def is_thanks(text: str) -> bool:
    """Check whether input text expresses gratitude."""
    t = normalize_fa(text).lower()
    return any(w in t for w in THANKS)


def generate_response(intent: str, sentiment: str, entities: dict, text: str) -> str:
    """
    Generate a professional, user-friendly response
    based on intent, sentiment, and extracted entities.
    """
    # Greeting / Thanks take priority over model prediction
    if is_greeting(text):
        return "Ø³Ù„Ø§Ù… ðŸ˜Š Ù…Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³ÛŒØ³ØªÙ… Ù…Ø¬ØªÙ…Ø¹ Ù‡Ø³ØªÙ…. Ù…Ø´Ú©Ù„ ÛŒØ§ Ø¯Ø±Ø®ÙˆØ§Ø³ØªØª Ø±Ùˆ Ø¨Ú¯Ùˆ ØªØ§ Ø³Ø±ÛŒØ¹ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒØª Ú©Ù†Ù…."

    if is_thanks(text):
        return "Ø®ÙˆØ§Ù‡Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù… ðŸŒ¿ Ø§Ú¯Ø± Ø¨Ø§Ø² Ù‡Ù… Ú©Ø§Ø±ÛŒ Ø¯Ø§Ø´ØªÛŒ Ø¯Ø± Ø®Ø¯Ù…ØªÙ…."

    # Support issue handling
    if intent == "support_issue":
        fac = ", ".join(entities.get("facility", [])) if entities.get("facility") else "Ù…Ø´Ú©Ù„ Ø§Ø¹Ù„Ø§Ù…â€ŒØ´Ø¯Ù‡"
        if sentiment == "negative":
            return f"Ù…ØªÙˆØ¬Ù‡ Ù†Ø§Ø±Ø§Ø­ØªÛŒ Ø´Ù…Ø§ Ù‡Ø³ØªÙ… ðŸ™ Ú¯Ø²Ø§Ø±Ø´ Ø®Ø±Ø§Ø¨ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ **{fac}** Ø«Ø¨Øª Ø´Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ ØªÛŒÙ… Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
        return f"âœ… Ú¯Ø²Ø§Ø±Ø´ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ **{fac}** Ø«Ø¨Øª Ø´Ø¯. Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† ÙØ±ØµØª Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯."

    # Facility reservation handling
    if intent == "facility_reservation":
        fac = ", ".join(entities.get("facility", [])) if entities.get("facility") else "Ø§Ù…Ú©Ø§Ù†Ø§Øª"
        date = ", ".join(entities.get("date", [])) if entities.get("date") else "Ø²Ù…Ø§Ù† Ù…ÙˆØ±Ø¯Ù†Ø¸Ø±"
        return f"âœ… Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±Ø²Ø±Ùˆ **{fac}** Ø¨Ø±Ø§ÛŒ **{date}** Ø«Ø¨Øª Ø´Ø¯. Ø§Ú¯Ø± Ø²Ù…Ø§Ù† Ø¯Ù‚ÛŒÙ‚ Ù‡Ù… Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ Ú©Ø§Ù…Ù„â€ŒØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯."

    # Financial inquiries
    if intent == "financial_inquiry":
        if sentiment == "negative":
            return "Ù…ØªÙˆØ¬Ù‡ Ù†Ú¯Ø±Ø§Ù†ÛŒ Ø´Ù…Ø§ Ù‡Ø³ØªÙ… ðŸ™ Ù„Ø·ÙØ§Ù‹ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ Ø¯Ø±Ø¨Ø§Ø±Ù‡ **Ø´Ø§Ø±Ú˜ØŒ Ø¨Ø¯Ù‡ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø®Øª** Ú©Ø¯Ø§Ù… Ù…ÙˆØ±Ø¯ Ø³ÙˆØ§Ù„ Ø¯Ø§Ø±ÛŒØ¯ØŸ"
        return "Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù…Ø§Ù„ÛŒØŒ Ù„Ø·ÙØ§Ù‹ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯: **Ø´Ø§Ø±Ú˜ Ø§ÛŒÙ† Ù…Ø§Ù‡ / Ø¨Ø¯Ù‡ÛŒ / ØªØ§ÛŒÛŒØ¯ Ù¾Ø±Ø¯Ø§Ø®Øª**ØŸ"

    # Operation / request status
    if intent == "operation_status":
        if sentiment == "negative":
            return "Ø­Ù‚ Ø¯Ø§Ø±ÛŒØ¯ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ú©Ù†ÛŒØ¯ ðŸ™ Ù„Ø·ÙØ§Ù‹ Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÛŒØ§ ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡ Ø¨Ø¯Ù‡ÛŒØ¯ ØªØ§ ÙˆØ¶Ø¹ÛŒØªØ´ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†Ù…."
        return "Ù„Ø·ÙØ§Ù‹ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ **Ú©Ø¯Ø§Ù… Ø¯Ø±Ø®ÙˆØ§Ø³Øª/Ø®Ø±Ø§Ø¨ÛŒ** Ø§Ø³Øª ØªØ§ ÙˆØ¶Ø¹ÛŒØª Ø¢Ù† Ø±Ø§ Ø§Ø¹Ù„Ø§Ù… Ú©Ù†Ù…."

    # Fallback response
    return "Ù…ØªÙˆØ¬Ù‡ Ù†Ø´Ø¯Ù… ðŸ˜… Ù„Ø·ÙØ§Ù‹ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ Ù…Ø´Ú©Ù„ Ø®Ø±Ø§Ø¨ÛŒ Ø§Ø³Øª ÛŒØ§ Ø±Ø²Ø±Ùˆ ÛŒØ§ Ù…Ø§Ù„ÛŒØŸ"


# -----------------------------
# 4) Main Chatbot Interface Class
# -----------------------------
class Chatbot:
    """
    High-level chatbot interface:
    - Loads trained model and tokenizer
    - Runs inference
    - Produces structured prediction results
    """
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.max_len = MAX_LEN

    def load_models(self):
        """
        Load trained model checkpoint and tokenizer vocabulary.
        """
        model_path = Path(MODEL_PATH)
        if not model_path.exists():
            raise FileNotFoundError(f"âš ï¸ Model file not found at {MODEL_PATH}. Train first.")

        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)

        self.model = ChatbotModel(vocab_size=len(checkpoint["vocab"])).to(DEVICE)
        self.model.load_state_dict(checkpoint["model_state"])
        self.model.eval()

        self.tokenizer = Tokenizer()
        self.tokenizer.word2idx = checkpoint["vocab"]
        self.max_len = checkpoint.get("max_len", MAX_LEN)

        print("âœ… Models loaded successfully")

    def predict(self, text: str) -> dict:
        """
        Run inference on input text and return:
        - intent
        - sentiment
        - probabilities
        - extracted entities
        - generated response
        """
        text = normalize_fa(text)

        if self.model is None or self.tokenizer is None:
            raise RuntimeError("Models not loaded. Please run load_models() first.")

        # Handle greeting explicitly to avoid misclassification
        if is_greeting(text):
            return {
                "intent": "greeting",
                "sentiment": "neutral",
                "intent_prob": [1.0, 0.0, 0.0, 0.0],
                "sentiment_prob": [0.0, 1.0, 0.0],
                "entities": {},
                "response_text": generate_response("greeting", "neutral", {}, text)
            }

        x = torch.tensor(
            [self.tokenizer.encode(text, self.max_len)],
            dtype=torch.long
        ).to(DEVICE)

        with torch.no_grad():
            intent_logits, sent_logits, _ = self.model(x)

        intent_prob = torch.softmax(intent_logits, dim=1)[0].cpu().tolist()
        sent_prob = torch.softmax(sent_logits, dim=1)[0].cpu().tolist()

        intent_idx = int(torch.argmax(intent_logits, dim=1).item())
        sent_idx = int(torch.argmax(sent_logits, dim=1).item())

        intent = INTENTS[intent_idx]
        sentiment = SENTIMENTS[sent_idx]

        entities = extract_entities(text)
        response_text = generate_response(intent, sentiment, entities, text)

        return {
            "intent": intent,
            "sentiment": sentiment,
            "intent_prob": intent_prob,
            "sentiment_prob": sent_prob,
            "entities": entities,
            "response_text": response_text
        }


# Ready-to-use singleton instance
chatbot = Chatbot()


def load_models():
    """Convenience wrapper for loading models globally."""
    chatbot.load_models()
